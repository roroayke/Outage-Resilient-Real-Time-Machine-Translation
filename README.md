# ‚ö° Efficient NMT via Knowledge Distillation (KD)

## üöÄ Project Overview

This project explores **Knowledge Distillation (KD)** to compress a large **MarianMT Teacher** model into a lightweight **Student** model for **German-English Neural Machine Translation (NMT)**. The goal: retain translation quality while improving inference speed, memory footprint, and energy efficiency ‚Äî all deployable on **consumer-grade GPUs like the RTX 4070**.

> üß† We distilled translations using sequence-level KD, benchmarked energy use, and showed that efficient, low-carbon machine translation is possible on local devices.

---

## üß™ Model Setup

| Component     | Description                                      |
|---------------|--------------------------------------------------|
| **Teacher**   | MarianMT (Helsinki-NLP, 6-layer Transformer)     |
| **Student**   | 3-layer Transformer (sequence-level distilled)   |
| **Training**  | Hugging Face + PyTorch + FP16 mixed-precision    |
| **Distillation** | Sequence-level only (no logits/intermediate layers) |

---

## üñ•Ô∏è Hardware Benchmarking

| Metric                 | RTX 4070        | Tesla T4         |
|------------------------|----------------|------------------|
| Training Time          | ~1.8 minutes    | ~0.6 minutes     |
| Cost (est.)            | ~$0.01          | ~$0.00           |
| BLEU (Teacher)         | 18.10           | 18.10            |
| BLEU (Student)         | 8.01            | 7.44             |
| Emissions Estimate     | << 0.001 kg CO‚ÇÇ | ~0.001 kg CO‚ÇÇ    |

üìâ **BLEU drop** shows the tradeoff with shallow KD + tiny data (~1k samples).  
üå± But: Carbon impact = almost nothing. Let‚Äôs call it **"Green Translation."**

---

## üìÇ Dataset

- **Language Pair**: German ‚Üî English
- **Corpus**: ~1,000 parallel sentence pairs
- **Tokenization**: Hugging Face pre-trained tokenizer
- **Note**: Tiny dataset = rapid prototyping, not full model fidelity

---

## üìà Results Summary

| Model Type         | BLEU Score | Token Overlap (%) |
|--------------------|------------|-------------------|
| Teacher (MarianMT) | 18.10      | --                |
| Student (KD Model) | 8.01       | ~9.4% avg         |

The student model:
- Preserved coarse structure
- Struggled with idioms, imperatives, short phrases
- Was **~2.5x faster** at inference (qualitatively)

---

## üåç Environmental Insights

We benchmarked GPU energy and cost efficiency using **consumer hardware**:
- ‚úÖ Trained in < 2 mins on RTX 4070
- ‚úÖ Cost under **$0.01**
- ‚úÖ CO‚ÇÇ emissions estimated at **~1000x lower** than baseline

> Efficiency isn‚Äôt just good engineering ‚Äî it‚Äôs ethical AI.

---
